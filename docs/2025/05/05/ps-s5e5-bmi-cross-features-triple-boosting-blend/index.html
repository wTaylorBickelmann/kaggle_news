<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Headline: Calories, Cross-Features and CatBoost: Inside the Unconventional Ensemble That Tops Kaggle - The Daily Bin</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>
        body { font-family: Arial, sans-serif; max-width: 900px; margin:0 auto; padding:20px; }
        header { margin-bottom: 40px; }
        header a.home { text-decoration:none; color:#666; font-size:0.9em; }
        header h1 { margin:10px 0 5px 0; font-size:2em; }
        .meta { color:#888; font-size:0.9em; margin-bottom:20px; }
        figure { margin:0 0 30px 0; }
        figure img { width:100%; border-radius:4px; }
        .content { line-height:1.6; color:#333; }
        .links { margin-top:40px; font-size:0.9em; }
        .links a { color:#0077cc; text-decoration:none; }
        footer { text-align:center; margin-top:60px; color:#888; font-size:0.8em; }
    </style>
</head>
<body>
<header>
    <a class="home" href="/">← Back to front page</a>
    <h1>Headline: Calories, Cross-Features and CatBoost: Inside the Unconventional Ensemble That Tops Kaggle</h1>
    <div class="meta">May 05, 2025 &middot; Source: kaggle.com</div>
</header>


<figure>
    <img src="2025/05/05/ps-s5e5-bmi-cross-features-triple-boosting-blend/cover.png" alt="Headline: Calories, Cross-Features and CatBoost: Inside the Unconventional Ensemble That Tops Kaggle">
</figure>


<div class="content">
    Headline: Calories, Cross-Features and CatBoost: Inside the Unconventional Ensemble That Tops Kaggle<br><br>In a recent Kaggle playground challenge to predict calorie expenditure, one data scientist fused classic health metrics with aggressive feature engineering and a three-way boosting ensemble to punch through the leaderboard. The notebook’s key innovation lies in its marriage of simple transforms—squaring and square-rooting age, height, weight, duration, heart rate and body temperature—with a full set of pairwise “cross” features (including the newly computed BMI). That’s dozens of extra predictors designed to capture subtle non-linear and interaction effects between variables.<br><br>Rather than relying on dimensionality reduction or automated feature selectors, the author leaned into raw feature proliferation. Categorical encoding of “Sex” was handled natively by CatBoost and LightGBM for speed and memory efficiency. The target variable (Calories) was log-transformed (“log1p”) to stabilize high variance and directly optimize for RMSLE. Three gradient-boosting models—CatBoostRegressor, XGBRegressor and LGBMRegressor—were then trained in a 5-fold cross-validation loop with early stopping, tuned learning rates and depth constraints.<br><br>What makes this approach stand out is its simplicity blended with brute-force creativity:  <br>• A massive polynomial feature set (including interaction terms with BMI) that would normally risk overfitting, but is tamed through cross-validation and regularization.  <br>• Exploiting native categorical support in two of the frameworks to squeeze more performance out of a single human-readable column.  <br>• A straightforward weighted blend (40% XGBoost, 30% CatBoost, 30% LightGBM) of out-of-fold predictions, followed by an inverse log transform and clipping to a reasonable range.<br><br>This intentionally over-parameterized, multi-model strategy delivered one of the competition’s top RMSLE scores, proving that in certain data-rich forecasting tasks, an “all-in” feature blow-up combined with a lightweight ensemble can beat more parsimonious or black-box automated pipelines.
</div>

<div class="links">
    <p><a href="https://www.kaggle.com/canozensoy/ps-s5e5-bmi-cross-features-triple-boosting-blend">Read original article</a> &middot; <a href="https://www.kaggle.com/canozensoy/ps-s5e5-bmi-cross-features-triple-boosting-blend">Open notebook on Kaggle</a></p>
</div>

<footer>
    Generated automatically via git-scraper.
</footer>
</body>
</html>