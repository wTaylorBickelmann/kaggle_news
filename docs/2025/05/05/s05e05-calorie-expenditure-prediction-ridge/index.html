<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Headline: “Double-Dataset Deep Dive: A Kaggle Notebook That Mines Mutual Information for a High-Octane Ensemble” - The Daily Bin</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>
        body { font-family: Arial, sans-serif; max-width: 900px; margin:0 auto; padding:20px; }
        header { margin-bottom: 40px; }
        header a.home { text-decoration:none; color:#666; font-size:0.9em; }
        header h1 { margin:10px 0 5px 0; font-size:2em; }
        .meta { color:#888; font-size:0.9em; margin-bottom:20px; }
        figure { margin:0 0 30px 0; }
        figure img { width:100%; border-radius:4px; }
        .content { line-height:1.6; color:#333; }
        .links { margin-top:40px; font-size:0.9em; }
        .links a { color:#0077cc; text-decoration:none; }
        footer { text-align:center; margin-top:60px; color:#888; font-size:0.8em; }
    </style>
</head>
<body>
<header>
    <a class="home" href="/">← Back to front page</a>
    <h1>Headline: “Double-Dataset Deep Dive: A Kaggle Notebook That Mines Mutual Information for a High-Octane Ensemble”</h1>
    <div class="meta">May 05, 2025 &middot; Source: kaggle.com</div>
</header>


<figure>
    <img src="2025/05/05/s05e05-calorie-expenditure-prediction-ridge/cover.png" alt="Headline: “Double-Dataset Deep Dive: A Kaggle Notebook That Mines Mutual Information for a High-Octane Ensemble”">
</figure>


<div class="content">
    Headline: “Double-Dataset Deep Dive: A Kaggle Notebook That Mines Mutual Information for a High-Octane Ensemble”<br><br>In a recent entry to the Playground Series, one Kaggle competitor took an unusual tack: rather than relying solely on the supplied “train.csv,” they pulled in an older, external “calories.csv” dataset from a past Kaggle competition to guide their feature-engineering. By computing mutual information scores and correlation heatmaps side-by-side on both datasets, they verified that the three core physiological features—Duration, Heart_Rate and Body_Temp—consistently carried the most predictive power (even outpacing age, height and weight). This cross-validation of feature relevance allowed them to proceed with confidence that their model wouldn’t be swayed by quirks in the playground data.<br><br>Once the signal-carrying features were locked in, the competitor launched a full ensemble offensive. They transformed the calorie target with a log1p to stabilize variance, then set up a 5-fold KFold split (shuffle=True, random_state=42) to ensure robust out-of-fold (OOF) metrics. Their base learners included scikit-learn’s HistGradientBoostingRegressor, LightGBM, XGBoost, CatBoost and even a Ridge regressor—each tuned via Optuna over 250 trials. Unusually large hyperparameter values surfaced from the search: HistGB with max_depth of 59, more than 4,400 boosting iterations and a fractional “max_features” of 0.31, as well as a LightGBM config boasting 50,000 trees and a reg_alpha of 2.03.<br><br>Training was orchestrated with the open-source “koolbox” Trainer, which streamlined trial management, early stopping and logging. In their final stack, the competitor blended all five models, averaging predictions in log space and then applying expm1 to return to the original calorie scale. The result? A cross-validated RMSE that landed comfortably in the top decile of leaderboard scores, demonstrating that thoughtful, cross-dataset feature selection—paired with relentless hyperparameter pursuit—can still be a winning formula on Kaggle.
</div>

<div class="links">
    <p><a href="https://www.kaggle.com/ravaghi/s05e05-calorie-expenditure-prediction-ridge">Read original article</a> &middot; <a href="https://www.kaggle.com/ravaghi/s05e05-calorie-expenditure-prediction-ridge">Open notebook on Kaggle</a></p>
</div>

<footer>
    Generated automatically via git-scraper.
</footer>
</body>
</html>